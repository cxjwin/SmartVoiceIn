我描述下背景
本来我是为了方便我更好的去做 Vibe Coding，所以尝试了很多语音输入法，包括Typeless，还有国产的闪电说，其实也包括了微信自带的语音输入法，就是微信自带的那些语音输入法的话，它占用了FN键。这些输入法的话，要不就是识别，呃，有点问题；，要不就是，呃，需要付费，而且也不便宜；，啊，要么就是占用了快捷键，啊，不能很方便地做自定义。所以我就想，既然现在做 Vibe coding 非常方便，那我是不是自己可以按自己的一些想法和思路去做一个语音输入法？一个想法。这里就诞生了做一个 Smart Voice In 的这样一个mac 态栏工具的一个想法。
同时，我尝试去用纯语音的方式去编写这篇 KM。一方面通过实战方式验证其输出准确性，同时看使用过程中是否有 bug。
一些项目中的思考。
为什么会考虑使用千问的本地模型？一开始我主要使用的是腾讯云 SR 和腾讯云的大模型，但发现因为它要过云端，所以速度存在一定的延时。
然后研究一下 ASR 领域哪个开源模型运行比较好，发现千问的一个 ASR 模型，包括一个 0.6B 的和一个 1.7B 的。测试下来，发现这两个模型对于整个 ASR 的识别精度还可以。当然，一点七 B 的效果要更好一点。但问题是，一点七 B 占用的空间也会更大。
在LLM优化这一块，也尝试了腾讯云，因为手边也有Mini Max的账号，同时也尝试使用了Mini Max。这里的整体API请求主要参考官方文档。那直接把文档链接丢给Codex，它就可以自动开始编程。整个过程是通过Vibe Code进行的。
其实后续也有一个思考，因为 ASR 推理一块的话是用了一个千问的开源模型，那我就想，对于 LLM 推理一块的话，我是不是也可以去使用千问的开源模型？这里直接就使用了一个体积比较小的语言处理模型。
目前来看，如果用纯本地模型，基本上可以满足语音输入的基本诉求。用纯本地模型的话，在那个输入输出的速度上有一个较好的保证。当然，前提是对那个电脑的配置和存储空间也有一定的要求。
但是要达到 Typeless 的优化效果，仅使用模型可能还行。这里的话，对这个整个提示词进行了优化和改造。整个提示值经过了很多版本的调优，但目前仍然存在一些瑕疵。不过，在绝大多数场景下，大约百分之八十的使用已经可以正常进行。
整个项目其实，中间不会去人为的去写，全都是通过大模型，通过 Vibe Coding 的方式去实现的。当然，这里面，基本上整个一个 App 的基础功能的话，其实两个晚上就已经搭建完成。后面其实花了很多时间去打磨一些细节和调优，然后不断的去加入过程中的一些思考和自己的想法，去让AI快速地去进行落地。
旧的编码方式，经过一个深度的思考，然后去进行一个实际的编码。那这里面很多时候，我的一种另外一种的方式，就是我去不停的去把我的想法通过口语的方式去把它表述出来，然后我的AI的工具去帮我生成代码。
很多时候，我是只需要做一个产品经理的角色，然后做一个测试的角色就可以了。
当然，中间也会呃穿插一些，比如说架构师的角色，因为中间有很多的功能的小的迭代，那已经最初的一个项目设计的架构已经不太符合，也不太适合他去做一个扩展了。那这里面我就要去通过一些设计模式啊，或者说一些小的重构，去让让他去扩展更多的一个功能啊，比如说我也提供了几个本地的 ASR 模型或者本地的 LLM 模型去让它运行，这样的话就提供了1个 provider 的设计模式。
另外1个就是核心点的话，就是那个快捷键的处理，因为默认的快捷键要不就是F 键，要不就是 Option ，比如说 Command 那些快捷键。当然，系键盘里面的 Option 、Command 又分左右，那右侧的快捷键其实通常用的比较少，所以我就这里面有思考的话，就是可以去定制这些快捷键。当然，最初中的想法就是，之前我用Typeless的时候，我的那个Mac的键盘是HHKB，那HHKB上的那个FN快捷键是TabList是识别不到的，所以这就阻碍了我进一步使用去使用这个App。后面呢也是触发了我去自定义1个App的想法，能够快速地自定义快捷键。
另外一层含义的话，就是，呃，春节期间其实出了很多大模型，包括一些国产的大模型，比如说 MiniMax 2.5，GLM 5，Qwen 3.5 Plus 等。那我也在里面去尝试了不同模型、不同 IDE 之间在项目中的混合的一些情况。

